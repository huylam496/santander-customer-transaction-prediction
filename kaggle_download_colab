
In this tutorial, I show how to download kaggle datasets into google colab. Kaggle has been and remains the de factor platform to try your hands on data science projects. The platform has huge rich free datasets for machine learning projects. Another product from google, the company behind kaggle is colab, a platform suitable for training machine learning models and deep neural network free of charge without any installation requirement. One key thing that makes colab a game changer, especially for people who do not own GPU laptop is that users have the option to train their models with free GPU. Colab does not have the trove of datasets kaggle host on its platform therefore, it will be nice if you could access the datasets on kaggle from colab. There is in fact a kaggle API which we can use in colab but setting it up to work is not so easy. I would want to show how to use the API in a few simple steps.

Step 1 Create a kaggle account if you do not have one already. Click on your user name, click on account and scroll down to click on create new API token. This will download a file unto your PC. Note the location of the downloaded file.

Step 2 Go to colab via this link: https://colab.research.google.com/notebooks/welcome.ipynb and under file, click on new python 3 notebook. In the first cell, type this code to install kaggle API and make a directory called kaggle.

In [0]:
!pip install -U -q kaggle
!mkdir -p ~/.kaggle
Step 3 Type this code into the next cell and run to import the API key into colab

In [0]:
from google.colab import files
files.upload()
In the next cell, copy the API key to the kaggle directory we created.

In [0]:
!cp kaggle.json ~/.kaggle/
The datasets should be available for us to use. Let us list the datasets with this code.

In [4]:
!kaggle datasets list
Warning: Your Kaggle API key is readable by otherusers on this system! To fix this, you can run'chmod 600 /root/.kaggle/kaggle.json'
ref                                                             title                                                size  lastUpdated          downloadCount  
--------------------------------------------------------------  --------------------------------------------------  -----  -------------------  -------------  
blastchar/telco-customer-churn                                  Telco Customer Churn                                172KB  2018-02-23 18:20:00           2557  
heesoo37/120-years-of-olympic-history-athletes-and-results      120 years of Olympic history: athletes and results    5MB  2018-06-15 06:10:41           8304  
neuromusic/avocado-prices                                       Avocado Prices                                      629KB  2018-06-06 05:28:35           3347  
bigquery/patents                                                Google Patents Public Data                            2TB  2018-08-24 18:21:31              0  
cityofLA/la-restaurant-market-health-data                       LA Restaurant & Market Health Data                   10MB  2018-08-17 20:29:28            963  
open-powerlifting/powerlifting-database                         Powerlifting Database                                 9MB  2018-02-02 16:42:51           2358  
meganrisdal/la-county-restaurant-inspections-and-violations     LA County Restaurant Inspections and Violations      19MB  2018-04-19 17:33:03            673  
mathan/fifa-2018-match-statistics                               Predict FIFA 2018 Man of the Match                    4KB  2018-07-18 08:29:22           4499  
decide-soluciones/air-quality-madrid                            Air Quality in Madrid (2001-2018)                   151MB  2018-06-18 13:18:22           2113  
city-of-seattle/seattle-trade-permits                           Seattle Trade Permits                                13MB  2018-07-24 02:07:32            228  
kevinarvai/clinvar-conflicting                                  Genetic Variant Classifications                       3MB  2018-04-22 19:16:07            887  
datafiniti/fast-food-restaurants                                Fast Food Restaurants Across America                663KB  2018-04-10 18:35:54           1841  
marklvl/bike-sharing-dataset                                    Bike Sharing in Washington D.C. Dataset             273KB  2018-05-28 11:31:09            931  
new-york-city/ny-daily-inmates-in-custody                       NY Daily Inmates In Custody                         319KB  2018-09-03 07:15:46            497  
lucidlenn/sloan-digital-sky-survey                              Sloan Digital Sky Survey RD14                       604KB  2018-03-09 11:03:45            428  
passnyc/data-science-for-good                                   PASSNYC: Data Science for Good Challenge            164KB  2018-06-26 17:36:48           6100  
4quant/depth-generation-lightfield-imaging                      Computational Imaging                               350MB  2018-07-10 16:58:22            280  
mirichoi0218/insurance                                           Medical Cost Personal Datasets                      16KB  2018-02-21 00:15:14           5210  
fernandol/countries-of-the-world                                Countries of the World                               13KB  2018-04-26 08:16:27           3733  
tadhgfitzgerald/fifa-international-soccer-mens-ranking-1993now  FIFA Soccer Rankings                                693KB  2018-06-08 11:15:09           3486  
Step 4 We can download files now by using this sample code. In this case the US consumer finance complaints was downloaded.

In [5]:
!kaggle datasets download -d cfpb/us-consumer-finance-complaints
!ls
Warning: Your Kaggle API key is readable by otherusers on this system! To fix this, you can run'chmod 600 /root/.kaggle/kaggle.json'
Downloading us-consumer-finance-complaints.zip to /content
 98%|█████████████████████████████████████▍| 89.0M/90.5M [00:02<00:00, 40.9MB/s]
100%|██████████████████████████████████████| 90.5M/90.5M [00:02<00:00, 38.5MB/s]
kaggle.json  sample_data  us-consumer-finance-complaints.zip
Step 5 We use pandas to read the data we have downloaded by unzipping the file first. This line of code works in most situations.

In [6]:
import pandas as pd
data2 = pd.read_csv('/content/us-consumer-finance-complaints.zip', compression='zip', header=0, sep=',', quotechar='"')
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-6-291f40351b47> in <module>()
      1 import pandas as pd
----> 2 data2 = pd.read_csv('/content/us-consumer-finance-complaints.zip', compression='zip', header=0, sep=',', quotechar='"')

/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)
    707                     skip_blank_lines=skip_blank_lines)
    708 
--> 709         return _read(filepath_or_buffer, kwds)
    710 
    711     parser_f.__name__ = name

/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds)
    447 
    448     # Create the parser.
--> 449     parser = TextFileReader(filepath_or_buffer, **kwds)
    450 
    451     if chunksize or iterator:

/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds)
    816             self.options['has_index_names'] = kwds['has_index_names']
    817 
--> 818         self._make_engine(self.engine)
    819 
    820     def close(self):

/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py in _make_engine(self, engine)
   1047     def _make_engine(self, engine='c'):
   1048         if engine == 'c':
-> 1049             self._engine = CParserWrapper(self.f, **self.options)
   1050         else:
   1051             if engine == 'python':

/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py in __init__(self, src, **kwds)
   1693         kwds['allow_leading_cols'] = self.index_col is not False
   1694 
-> 1695         self._reader = parsers.TextReader(src, **kwds)
   1696 
   1697         # XXX

pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__()

pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._setup_parser_source()

ValueError: ('Multiple files found in compressed zip file %s', "['consumer_complaints.csv', 'database.sqlite']")
It did not work here because the zipped file also contains a sqlite database. I will use a different method below to extract only the CSV.

In [7]:
from zipfile import ZipFile
zip_file = ZipFile('/content/us-consumer-finance-complaints.zip')
fields= ['product','consumer_complaint_narrative'] 
data=pd.read_csv(zip_file.open('consumer_complaints.csv'), usecols=fields)
data.head()
/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.
  interactivity=interactivity, compiler=compiler, result=result)
Out[7]:
product	consumer_complaint_narrative
0	Mortgage	NaN
1	Mortgage	NaN
2	Credit reporting	NaN
3	Student loan	NaN
4	Debt collection	NaN
In [0]:
